{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c6015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jages\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8eceeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessDataScraper:\n",
    "    def __init__(self, base_url: str, max_retries: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the web scraper with configuration options\n",
    "        \n",
    "        Args:\n",
    "            base_url (str): Base URL to scrape\n",
    "            max_retries (int): Maximum number of retry attempts for requests\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = max_retries\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Accept-Language': 'en-US,en;q=0.9'\n",
    "        }\n",
    "        logging.basicConfig(level=logging.INFO, \n",
    "                            format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def make_request(self, url: str) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Make a robust web request with retry mechanism\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL to request\n",
    "        \n",
    "        Returns:\n",
    "            requests.Response: Web page response\n",
    "        \"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except requests.RequestException as e:\n",
    "                self.logger.warning(f\"Request failed (Attempt {attempt + 1}): {e}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        raise RuntimeError(f\"Failed to retrieve {url} after {self.max_retries} attempts\")\n",
    "\n",
    "    def extract_business_data(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract structured business data from BeautifulSoup object\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML content\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: Extracted business information\n",
    "        \"\"\"\n",
    "        business_data = []\n",
    "        company_elements = soup.find_all('div', class_='companyCardWrapper__primaryInformation')\n",
    "        \n",
    "        for element in company_elements:\n",
    "            try:\n",
    "                company_name = element.find('h2', class_='companyCardWrapper__companyName').text.strip()\n",
    "                rating = element.find('div', class_='rating_star_container').text.strip()\n",
    "                domain_location = element.find('span', class_='companyCardWrapper__interLinking').text.strip()\n",
    "                \n",
    "                business_data.append({\n",
    "                    'name': company_name,\n",
    "                    'rating': rating,\n",
    "                    'domain_location': domain_location\n",
    "                })\n",
    "            except AttributeError as e:\n",
    "                self.logger.warning(f\"Could not extract full data for an element: {e}\")\n",
    "        \n",
    "        return business_data\n",
    "\n",
    "    def scrape_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main scraping method to collect and process business data\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Cleaned and processed business data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.make_request(self.base_url)\n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "            \n",
    "            raw_data = self.extract_business_data(soup)\n",
    "            df = pd.DataFrame(raw_data)\n",
    "            \n",
    "            # Data Cleaning\n",
    "            df.dropna(subset=['name'], inplace=True)\n",
    "            df.drop_duplicates(subset=['name'], keep='first', inplace=True)\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Scraping failed: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def save_data(self, df: pd.DataFrame, filename: str = 'business_data.csv'):\n",
    "        \"\"\"\n",
    "        Save scraped data to CSV with anonymization\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to save\n",
    "            filename (str): Output filename\n",
    "        \"\"\"\n",
    "        # Optional: Add anonymization steps\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        self.logger.info(f\"Data saved to {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d575d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    scraper = BusinessDataScraper('https://www.ambitionbox.com/list-of-companies')\n",
    "    business_data = scraper.scrape_data()\n",
    "    scraper.save_data(business_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b259a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 11:08:04,945 - INFO: Data saved to business_data.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a77131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
